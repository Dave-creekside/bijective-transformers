# Baseline training configuration for discrete diffusion
# Conservative settings for stable initial training

# Training schedule
num_epochs: 50
max_steps: null  # If set, overrides num_epochs
warmup_steps: 1000
save_every_n_epochs: 5
eval_every_n_epochs: 1

# Optimizer settings
optimizer:
  type: "adamw"
  lr: 5e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning rate scheduler
lr_scheduler:
  type: "cosine"  # linear, cosine, polynomial, constant
  warmup_type: "linear"
  min_lr: 1e-6
  cycle_length: null  # For cosine annealing

# Loss configuration
loss:
  type: "cross_entropy"
  label_smoothing: 0.1
  ignore_index: -100
  reduction: "mean"

# Gradient settings
gradient_clipping:
  enabled: true
  max_norm: 1.0
  norm_type: 2

# Mixed precision training
mixed_precision:
  enabled: true
  dtype: "float16"  # float16, bfloat16

# Regularization
regularization:
  dropout: 0.1
  weight_decay: 0.01
  layer_decay: null  # Layer-wise learning rate decay

# Checkpointing
checkpointing:
  save_best: true
  save_last: true
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3

# Early stopping
early_stopping:
  enabled: true
  patience: 10
  monitor: "val_loss"
  mode: "min"
  min_delta: 1e-4

# Validation
validation:
  check_val_every_n_epoch: 1
  val_check_interval: null  # Check within epoch if set
  limit_val_batches: null  # Limit validation batches for speed

# Logging
logging:
  log_every_n_steps: 100
  log_grad_norm: true
  log_learning_rate: true
  log_loss_components: true

# Reproducibility
deterministic: false  # Set to true for full reproducibility (slower)
benchmark: true  # Set to false if input sizes vary significantly

# Compilation (PyTorch 2.0+)
compile:
  enabled: false  # Enable when stable
  mode: "default"  # default, reduce-overhead, max-autotune

# Profiling
profiling:
  enabled: false
  schedule: "pytorch"  # pytorch, simple
  activities: ["cpu", "cuda"]
  record_shapes: true
