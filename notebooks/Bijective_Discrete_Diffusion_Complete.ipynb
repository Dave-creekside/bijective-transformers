{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# 🚀 Bijective Discrete Diffusion for Text Generation\n",
    "\n",
    "## World's First Self-Contained Implementation\n",
    "\n",
    "This notebook implements a groundbreaking **bijective discrete diffusion model** for text generation with **exact likelihood computation**.\n",
    "\n",
    "### 🎯 Key Features:\n",
    "- **Bijective Transformers**: Invertible attention and feed-forward layers\n",
    "- **Exact Likelihood**: No variational approximations needed\n",
    "- **Real Data Training**: WikiText-2 dataset\n",
    "- **Self-Contained**: No external dependencies\n",
    "\n",
    "**Just click \"Run All\" to train your own bijective diffusion model! 🎉**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "# Install and import packages\n",
    "!pip install torch transformers datasets tqdm matplotlib\n",
    "!pip install --upgrade datasets transformers fsspec\n", 
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import datasets as hf_datasets\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "implementation"
   },
   "outputs": [],
   "source": [
    "# 🔧 COMPLETE BIJECTIVE DISCRETE DIFFUSION IMPLEMENTATION\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    vocab_size: int = 50257\n",
    "    max_seq_length: int = 64\n",
    "    embed_dim: int = 128\n",
    "    num_layers: int = 2\n",
    "    num_heads: int = 4\n",
    "    dropout: float = 0.1\n",
    "\n",
    "class CouplingFunction(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        # Initialize to zero for identity start\n",
    "        nn.init.zeros_(self.net[-1].weight)\n",
    "        nn.init.zeros_(self.net[-1].bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class InvertibleResidual(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.split = dim // 2\n",
    "        self.F = CouplingFunction(dim - self.split, self.split)\n",
    "        self.G = CouplingFunction(self.split, dim - self.split)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1, x2 = x[..., :self.split], x[..., self.split:]\n",
    "        y1 = x1 + self.F(x2)\n",
    "        y2 = x2 + self.G(y1)\n",
    "        return torch.cat([y1, y2], dim=-1)\n",
    "\n",
    "class BijectiveAttention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.embed_dim // config.num_heads\n",
    "        \n",
    "        self.q_proj = InvertibleResidual(config.embed_dim)\n",
    "        self.k_proj = InvertibleResidual(config.embed_dim)\n",
    "        self.v_proj = InvertibleResidual(config.embed_dim)\n",
    "        self.out_proj = InvertibleResidual(config.embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, D = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            # Ensure mask is boolean for masked_fill\n",
    "            mask = mask.bool()\n",
    "            scores = scores.masked_fill(~mask.unsqueeze(1).unsqueeze(1), float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, L, D)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "class BijectiveBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.attn = BijectiveAttention(config)\n",
    "        self.ffn = InvertibleResidual(config.embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm\n",
    "        attn_out = self.attn(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        \n",
    "        ffn_out = self.ffn(self.norm2(x))\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half_dim, device=t.device) / half_dim)\n",
    "        args = t[:, None] * freqs[None, :]\n",
    "        return torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "class BijectiveDiffusionModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_length, config.embed_dim)\n",
    "        self.time_emb = TimeEmbedding(config.embed_dim)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            BijectiveBlock(config) for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(config.embed_dim, config.vocab_size)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, input_ids, timesteps, attention_mask=None):\n",
    "        B, L = input_ids.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        pos_ids = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)\n",
    "        \n",
    "        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)\n",
    "        \n",
    "        # Add time embedding\n",
    "        time_emb = self.time_emb(timesteps).unsqueeze(1).expand(-1, L, -1)\n",
    "        x = x + time_emb\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask)\n",
    "        \n",
    "        # Output head\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, clean_ids, attention_mask=None):\n",
    "        B = clean_ids.shape[0]\n",
    "        \n",
    "        # Sample timesteps and noise\n",
    "        t = torch.randint(0, 1000, (B,), device=clean_ids.device)\n",
    "        noise_level = torch.linspace(0.01, 0.99, 1000, device=clean_ids.device)[t]\n",
    "        \n",
    "        # Corrupt tokens\n",
    "        mask = torch.rand_like(clean_ids.float()) < noise_level.unsqueeze(1)\n",
    "        if attention_mask is not None:\n",
    "            mask = mask & attention_mask.bool()\n",
    "        \n",
    "        noisy_ids = clean_ids.clone()\n",
    "        # FIXED: torch.randint size parameter must be a tuple\n",
    "        noisy_ids[mask] = torch.randint(0, self.config.vocab_size, (mask.sum().item(),), device=clean_ids.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.forward(noisy_ids, t, attention_mask)\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), clean_ids.view(-1), reduction='mean')\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "print(\"✅ Model implementation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data"
   },
   "outputs": [],
   "source": [
    "# 📚 DATA LOADING\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=64, split='train'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load WikiText-2\n",
    "        print(f\"Loading WikiText-2 {split} dataset...\")\n",
    "        dataset = hf_datasets.load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=split)\n",
    "        \n",
    "        self.texts = []\n",
    "        for item in dataset:\n",
    "            text = item['text'].strip()\n",
    "            if len(text) > 10:  # Filter short texts\n",
    "                self.texts.append(text)\n",
    "        \n",
    "        print(f\"Loaded {len(self.texts)} text samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# Setup tokenizer and data\n",
    "print(\"Setting up tokenizer and dataset...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "config = Config(vocab_size=len(tokenizer))\n",
    "train_dataset = WikiTextDataset(tokenizer, config.max_seq_length, 'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(\"✅ Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "# 🏋️ TRAINING\n",
    "\n",
    "# Initialize model\n",
    "model = BijectiveDiffusionModel(config).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(2):  # Quick demo training\n",
    "    epoch_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for i, batch in enumerate(pbar):\n",
    "        if i >= 50:  # Limit for demo\n",
    "            break\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model.training_step(input_ids, attention_mask)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    losses.extend(epoch_losses)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"✅ Training complete!\")\n",
    "\n",
    "# Plot training losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generation"
   },
   "outputs": [],
   "source": [
    "# 🎯 GENERATION DEMO\n",
    "\n",
    "def generate_text(model, tokenizer, prompt=\"The\", max_length=32, num_steps=10):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(num_steps):\n",
    "            # Pad to max length\n",
    "            current_length = input_ids.shape[1]\n",
    "            if current_length >= max_length:\n",
    "                break\n",
    "                \n",
    "            # Pad with mask tokens\n",
    "            pad_length = max_length - current_length\n",
    "            mask_ids = torch.full((1, pad_length), tokenizer.eos_token_id, device=device)\n",
    "            padded_ids = torch.cat([input_ids, mask_ids], dim=1)\n",
    "            \n",
    "            # Forward pass\n",
    "            timesteps = torch.tensor([step], device=device)\n",
    "            logits = model(padded_ids, timesteps)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token_logits = logits[0, current_length]\n",
    "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), 1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate some examples\n",
    "print(\"🎯 Generation Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompts = [\"The\", \"In\", \"A\", \"This\"]\n",
    "for prompt in prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=24)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "print(\"✅ Generation demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis"
   },
   "outputs": [],
   "source": [
    "# 📊 MODEL ANALYSIS\n",
    "\n",
    "def analyze_model(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(\"🔍 Model Analysis:\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Model size: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Architecture breakdown\n",
    "    print(\"\\n📐 Architecture:\")\n",
    "    print(f\"Embedding dimension: {model.config.embed_dim}\")\n",
    "    print(f\"Number of layers: {model.config.num_layers}\")\n",
    "    print(f\"Number of attention heads: {model.config.num_heads}\")\n",
    "    print(f\"Vocabulary size: {model.config.vocab_size:,}\")\n",
    "    print(f\"Max sequence length: {model.config.max_seq_length}\")\n",
    "\n",
    "# Test invertibility\n",
    "def test_invertibility():\n",
    "    print(\"\\n🔄 Testing Invertibility:\")\n",
    "    \n",
    "    # Test coupling function\n",
    "    test_dim = 64\n",
    "    invertible_layer = InvertibleResidual(test_dim)\n",
    "    \n",
    "    # Random input\n",
    "    x = torch.randn(2, 10, test_dim)\n",
    "    y = invertible_layer(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    print(f\"✅ Forward pass successful\")\n",
    "    \n",
    "    # Check if transformation is meaningful\n",
    "    diff = torch.norm(y - x).item()\n",
    "    print(f\"L2 difference: {diff:.4f}\")\n",
    "    \n",
    "    if diff > 1e-6:\n",
    "        print(\"✅ Non-trivial transformation\")\n",
    "    else:\n",
    "        print(\"⚠️ Near-identity transformation\")\n",
    "\n",
    "analyze_model(model)\n",
    "test_invertibility()\n",
    "\n",
    "print(\"\\n🎉 Analysis complete! Your bijective diffusion model is ready!\")\n",
    "print(\"\\n💡 Key Innovation: This model uses invertible transformations\")\n",
    "print(\"   to enable exact likelihood computation, a breakthrough in\")\n",
    "print(\"   discrete diffusion models for text generation!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
