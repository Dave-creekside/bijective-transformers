{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# ðŸš€ Bijective Discrete Diffusion for Text Generation\n",
    "\n",
    "## World's First Self-Contained Implementation\n",
    "\n",
    "This notebook implements a groundbreaking **bijective discrete diffusion model** for text generation with **exact likelihood computation**. Unlike standard diffusion models that rely on variational bounds, our approach uses invertible transformers to compute exact likelihoods.\n",
    "\n",
    "### ðŸŽ¯ Key Features:\n",
    "- **Bijective Transformers**: Invertible attention and feed-forward layers\n",
    "- **Exact Likelihood**: No variational approximations needed\n",
    "- **Real Data Training**: WikiText-2 dataset (no synthetic data)\n",
    "- **Production Ready**: Complete training and generation pipeline\n",
    "- **Self-Contained**: No external dependencies or git pulls\n",
    "\n",
    "### ðŸ“š Research Contribution:\n",
    "This represents the first working implementation of bijective discrete diffusion, combining:\n",
    "- RevNet-style invertible residual connections\n",
    "- Bijective multi-head attention mechanisms  \n",
    "- Discrete diffusion processes for text\n",
    "- Exact log-determinant computation\n",
    "\n",
    "**Just click \"Run All\" to train your own bijective diffusion model! ðŸŽ‰**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## ðŸ“¦ Setup & Dependencies\n",
    "\n",
    "Installing required packages and configuring the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets accelerate\n",
