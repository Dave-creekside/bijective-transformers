#!/usr/bin/env python3
"""
Test script for bijective transformer components.
Validates bijective linear layers, attention, and complete transformer blocks.
"""

import torch
import torch.nn.functional as F
from src.models.bijective_transformer import (
    BijectiveLinear,
    BijectiveMultiHeadAttention,
    BijectiveTransformerBlock,
    BijectiveTransformerConfig,
    create_bijective_transformer_config
)
from src.utils.invertibility import test_invertibility, create_test_config

def test_bijective_linear():
    """Test bijective linear transformation."""
    print("üîß Testing Bijective Linear...")
    
    embed_dim = 256
    batch_size, seq_len = 4, 32
    
    # Create bijective linear layer
    linear = BijectiveLinear(embed_dim, embed_dim, bias=True)
    
    # Test forward and inverse
    x = torch.randn(batch_size, seq_len, embed_dim)
    
    with torch.no_grad():
        # Forward pass
        y, log_det_forward = linear.forward(x)
        
        # Inverse pass
        x_reconstructed, log_det_inverse = linear.inverse(y)
        
        # Check reconstruction
        reconstruction_error = torch.norm(x - x_reconstructed, dim=-1).max().item()
        
        # Check log determinant consistency
        log_det_consistency = torch.abs(log_det_forward + log_det_inverse).max().item()
    
    assert y.shape == x.shape, f"Output shape mismatch: {y.shape} vs {x.shape}"
    assert reconstruction_error < 1e-6, f"Reconstruction error too large: {reconstruction_error}"
    assert log_det_consistency < 1e-6, f"Log det inconsistency: {log_det_consistency}"
    
    print(f"   ‚úÖ Forward/inverse shapes: {x.shape} -> {y.shape} -> {x_reconstructed.shape}")
    print(f"   ‚úÖ Reconstruction error: {reconstruction_error:.2e}")
    print(f"   ‚úÖ Log det consistency: {log_det_consistency:.2e}")
    
    return True

def test_bijective_attention():
    """Test bijective multi-head attention."""
    print("üîß Testing Bijective Multi-Head Attention...")
    
    # Create configuration
    config = create_bijective_transformer_config(
        embed_dim=256,
        num_heads=8,
        dropout=0.1,
        attention_dropout=0.1
    )
    
    # Create attention layer
    attention = BijectiveMultiHeadAttention(config)
    
    # Test forward and inverse
    batch_size, seq_len = 4, 32
    x = torch.randn(batch_size, seq_len, config.embed_dim)
    
    with torch.no_grad():
        # Forward pass with cache
        output, log_det_forward = attention.forward(x, store_cache=True)
        
        # Inverse pass using cache
        try:
            x_reconstructed, log_det_inverse = attention.inverse(output, use_cache=True)
            
            # Check reconstruction (will be approximate due to attention)
            reconstruction_error = torch.norm(x - x_reconstructed, dim=-1).max().item()
            
            print(f"   ‚úÖ Forward/inverse shapes: {x.shape} -> {output.shape} -> {x_reconstructed.shape}")
            print(f"   ‚úÖ Reconstruction error: {reconstruction_error:.2e} (approximate)")
            print(f"   ‚úÖ Log det forward: {log_det_forward.mean().item():.2e}")
            print(f"   ‚ö†Ô∏è  Attention inverse is approximate due to softmax")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Inverse attention error (expected): {e}")
            print(f"   ‚úÖ Forward pass working: {output.shape}")
            print(f"   ‚úÖ Log det forward: {log_det_forward.mean().item():.2e}")
    
    assert output.shape == x.shape, f"Output shape mismatch: {output.shape} vs {x.shape}"
    assert torch.isfinite(log_det_forward).all(), "Log determinant contains non-finite values"
    
    return True

def test_bijective_transformer_block():
    """Test complete bijective transformer block."""
    print("üîß Testing Bijective Transformer Block...")
    
    # Create configuration
    config = create_bijective_transformer_config(
        embed_dim=256,
        num_heads=8,
        dropout=0.1,
        use_bijective_attention=True,
        use_bijective_ffn=True,
        use_bijective_residuals=False  # Simplified for now
    )
    
    # Create transformer block
    block = BijectiveTransformerBlock(config)
    
    # Test forward pass
    batch_size, seq_len = 4, 32
    x = torch.randn(batch_size, seq_len, config.embed_dim)
    
    with torch.no_grad():
        # Forward pass
        output, log_det = block.forward(x)
        
        # Check output
        assert output.shape == x.shape, f"Output shape mismatch: {output.shape} vs {x.shape}"
        assert torch.isfinite(log_det).all(), "Log determinant contains non-finite values"
    
    print(f"   ‚úÖ Forward pass shapes: {x.shape} -> {output.shape}")
    print(f"   ‚úÖ Log determinant: {log_det.mean().item():.2e}")
    print(f"   ‚ö†Ô∏è  Full inverse not yet implemented")
    
    return True

def test_gradient_flow():
    """Test gradient flow through bijective transformer components."""
    print("üîß Testing Gradient Flow...")
    
    # Test bijective linear
    embed_dim = 128
    linear = BijectiveLinear(embed_dim, embed_dim)
    
    x = torch.randn(2, 16, embed_dim, requires_grad=True)
    y, log_det = linear.forward(x)
    loss = y.sum() + log_det.sum()
    loss.backward()
    
    assert x.grad is not None, "No gradients for bijective linear input"
    assert torch.isfinite(x.grad).all(), "Non-finite gradients in bijective linear"
    
    linear_grad_norm = x.grad.norm().item()
    
    # Test bijective attention
    config = create_bijective_transformer_config(embed_dim=128, num_heads=4)
    attention = BijectiveMultiHeadAttention(config)
    
    x2 = torch.randn(2, 16, embed_dim, requires_grad=True)
    output, log_det = attention.forward(x2)
    loss = output.sum() + log_det.sum()
    loss.backward()
    
    assert x2.grad is not None, "No gradients for attention input"
    assert torch.isfinite(x2.grad).all(), "Non-finite gradients in attention"
    
    attention_grad_norm = x2.grad.norm().item()
    
    print(f"   ‚úÖ Bijective linear gradient norm: {linear_grad_norm:.2e}")
    print(f"   ‚úÖ Bijective attention gradient norm: {attention_grad_norm:.2e}")
    
    return True

def test_configuration_options():
    """Test different configuration options."""
    print("üîß Testing Configuration Options...")
    
    embed_dim = 128
    num_heads = 4
    
    # Test different configurations
    configs = [
        {"use_bijective_attention": True, "use_bijective_ffn": True},
        {"use_bijective_attention": True, "use_bijective_ffn": False},
        {"use_bijective_attention": False, "use_bijective_ffn": True},
        {"use_bijective_attention": False, "use_bijective_ffn": False},
    ]
    
    for i, config_kwargs in enumerate(configs):
        config = create_bijective_transformer_config(
            embed_dim=embed_dim,
            num_heads=num_heads,
            **config_kwargs
        )
        
        try:
            block = BijectiveTransformerBlock(config)
            x = torch.randn(2, 16, embed_dim)
            
            with torch.no_grad():
                output, log_det = block.forward(x)
            
            assert output.shape == x.shape
            print(f"   ‚úÖ Config {i+1}: {config_kwargs} - Working")
            
        except Exception as e:
            print(f"   ‚ùå Config {i+1}: {config_kwargs} - Failed: {e}")
            return False
    
    return True

def test_memory_efficiency():
    """Test memory efficiency of bijective transformer components."""
    print("üîß Testing Memory Efficiency...")
    
    # Test with larger dimensions
    config = create_bijective_transformer_config(
        embed_dim=512,
        num_heads=8,
        dropout=0.1
    )
    
    block = BijectiveTransformerBlock(config)
    
    # Test with larger batch
    batch_size, seq_len = 8, 64
    x = torch.randn(batch_size, seq_len, config.embed_dim)
    
    import gc
    gc.collect()
    
    with torch.no_grad():
        output, log_det = block.forward(x)
    
    assert output.shape == x.shape
    
    print(f"   ‚úÖ Large batch processing: {x.shape}")
    print(f"   ‚úÖ Output shape: {output.shape}")
    print(f"   ‚úÖ Memory test completed")
    
    return True

def test_device_compatibility():
    """Test device compatibility for bijective transformer components."""
    print("üîß Testing Device Compatibility...")
    
    # Check device availability
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print(f"   ‚úÖ Using MPS device")
    else:
        device = torch.device("cpu")
        print(f"   ‚ö†Ô∏è  MPS not available, using CPU")
    
    try:
        # Create bijective linear layer on device
        embed_dim = 128
        linear = BijectiveLinear(embed_dim, embed_dim).to(device)
        
        # Test on device
        batch_size, seq_len = 2, 16
        x = torch.randn(batch_size, seq_len, embed_dim, device=device)
        
        with torch.no_grad():
            y, log_det = linear.forward(x)
            x_reconstructed, _ = linear.inverse(y)
        
        # Check reconstruction
        error = torch.norm(x - x_reconstructed, dim=-1).max().item()
        assert error < 1e-6, f"Reconstruction error on {device}: {error}"
        
        # Test bijective attention on device
        config = create_bijective_transformer_config(embed_dim=embed_dim, num_heads=4)
        attention = BijectiveMultiHeadAttention(config).to(device)
        
        with torch.no_grad():
            output, log_det = attention.forward(x)
        
        assert output.device.type == device.type
        assert log_det.device.type == device.type
        
        print(f"   ‚úÖ Bijective linear runs on {device}")
        print(f"   ‚úÖ Bijective attention runs on {device}")
        print(f"   ‚úÖ Reconstruction error: {error:.2e}")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Device compatibility error: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parameter_count():
    """Test parameter count of bijective vs standard components."""
    print("üîß Testing Parameter Count...")
    
    embed_dim = 256
    num_heads = 8
    
    # Bijective transformer block
    bijective_config = create_bijective_transformer_config(
        embed_dim=embed_dim,
        num_heads=num_heads,
        use_bijective_attention=True,
        use_bijective_ffn=True
    )
    bijective_block = BijectiveTransformerBlock(bijective_config)
    
    # Standard transformer block (approximation)
    standard_config = create_bijective_transformer_config(
        embed_dim=embed_dim,
        num_heads=num_heads,
        use_bijective_attention=False,
        use_bijective_ffn=False
    )
    standard_block = BijectiveTransformerBlock(standard_config)
    
    # Count parameters
    bijective_params = sum(p.numel() for p in bijective_block.parameters())
    standard_params = sum(p.numel() for p in standard_block.parameters())
    
    ratio = bijective_params / standard_params if standard_params > 0 else float('inf')
    
    print(f"   ‚úÖ Bijective block parameters: {bijective_params:,}")
    print(f"   ‚úÖ Standard block parameters: {standard_params:,}")
    print(f"   ‚úÖ Parameter ratio (bijective/standard): {ratio:.2f}x")
    
    return True

def test_attention_patterns():
    """Test attention pattern preservation."""
    print("üîß Testing Attention Patterns...")
    
    config = create_bijective_transformer_config(
        embed_dim=128,
        num_heads=4,
        attention_dropout=0.0  # No dropout for deterministic test
    )
    
    attention = BijectiveMultiHeadAttention(config)
    attention.eval()  # Disable dropout
    
    # Create input with clear patterns
    batch_size, seq_len = 1, 8
    x = torch.randn(batch_size, seq_len, config.embed_dim)
    
    with torch.no_grad():
        # Forward pass
        output1, log_det1 = attention.forward(x, store_cache=True)
        
        # Second forward pass with same input
        output2, log_det2 = attention.forward(x, store_cache=True)
        
        # Check determinism
        output_diff = torch.norm(output1 - output2).item()
        log_det_diff = torch.abs(log_det1 - log_det2).max().item()
    
    print(f"   ‚úÖ Output determinism: {output_diff:.2e}")
    print(f"   ‚úÖ Log det determinism: {log_det_diff:.2e}")
    print(f"   ‚úÖ Attention patterns preserved")
    
    return True

def main():
    """Run all bijective transformer tests."""
    print("üöÄ Bijective Transformer Tests")
    print("=" * 60)
    
    tests = [
        ("Bijective Linear", test_bijective_linear),
        ("Bijective Multi-Head Attention", test_bijective_attention),
        ("Bijective Transformer Block", test_bijective_transformer_block),
        ("Gradient Flow", test_gradient_flow),
        ("Configuration Options", test_configuration_options),
        ("Memory Efficiency", test_memory_efficiency),
        ("Device Compatibility", test_device_compatibility),
        ("Parameter Count", test_parameter_count),
        ("Attention Patterns", test_attention_patterns),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            print(f"\n{test_name}:")
            success = test_func()
            if success:
                passed += 1
                print(f"   ‚úÖ {test_name} PASSED")
            else:
                failed += 1
                print(f"   ‚ùå {test_name} FAILED")
        except Exception as e:
            failed += 1
            print(f"   ‚ùå {test_name} FAILED: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"üìä Test Results: {passed}/{len(tests)} passed")
    
    if failed == 0:
        print("üéâ All bijective transformer tests passed!")
        print("\nüöÄ Ready for:")
        print("   ‚Ä¢ Integration with discrete diffusion model")
        print("   ‚Ä¢ Performance benchmarking vs standard transformers")
        print("   ‚Ä¢ Full bijective transformer architecture")
    else:
        print(f"‚ö†Ô∏è  {failed} tests failed. Please review the errors above.")
        print("\nNote: Some limitations are expected:")
        print("   ‚Ä¢ Attention inverse is approximate due to softmax")
        print("   ‚Ä¢ Full transformer block inverse not yet implemented")
    
    return failed == 0

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
